# -*- coding: utf-8 -*-
"""Final_Code_SephoraProductAnalysis.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1rIunOSnQ7PxhPUlbxMwiQVPveVySJ-um
"""

#! pip install gensim
#! pip install vaderSentiment
#! pip install lda
#! pip install pyLDAvis

import pandas as pd
import numpy as np
import gensim
from gensim.utils import simple_preprocess
from gensim.parsing.preprocessing import STOPWORDS
from nltk.stem import WordNetLemmatizer, SnowballStemmer
from nltk.stem.porter import *
import numpy as np
from sklearn.feature_extraction.text import CountVectorizer
from sklearn.decomposition import LatentDirichletAllocation as LDA
import nltk
nltk.download('wordnet')
from sklearn.feature_extraction.text import CountVectorizer
from sklearn.decomposition import LatentDirichletAllocation as LDA

"""## Data Pre-processing"""

def preprocess_data(df):
    #Extact the minimum price from the price range
    df['min_price']=df['price'].str.split(expand=True)[0].str.replace('$','')
    df['min_price']=df['min_price'].astype('float')
    #preprocessed_items['min_price'].str.replace('$','')
    #format ratings columns - 
    # 1) Remove 'stars' from the column
    # 2) consider 'no rating' as 0 rating.
    df['rating']=df['rating'].str.split(expand=True)[0]
    df['rating'] = np.where(df['rating']=='No', 0 , df['rating'])
    # format the review counts - 
    # 1) Remove HTML Tags and replace with 0 count, 
    # 2) Replace K with 1000s
    df['review_count']= df['review_count'].astype('string')
    df['review_count']= np.where(df['review_count'].str.contains('class'), '0', df['review_count'])
    df['review_count']= np.where(df['review_count'].str.contains('K'), df['review_count'].str.replace('K','').astype(float)*1000, df['review_count'])
    df['review_count']=df['review_count'].astype(int)
    #Preprcess Data and create a new columns for processed reviews
    items["reviews_combined"]= items["reviews"].astype(str)
    df['reviews_combined']= df['reviews_combined'].apply(preprocess_text)
    return df

def lemmatize_stemming(text):  
    return WordNetLemmatizer().lemmatize(text, pos='v')
def preprocess_text(text):
    result = []
    #convert to lower case and remove unicode chars
    text=text.lower().encode('ascii', 'ignore').decode()
    #extract alphanumeic character
    text = re.sub('[^0-9a-zA-Z]', ' ', text)
    #Remove Stop words and lemmatize 
    for token in gensim.utils.simple_preprocess(text):
        if token not in gensim.parsing.preprocessing.STOPWORDS and len(token) > 3:
            result.append(lemmatize_stemming(token))
    return ' '.join(result)

#Import scarpes reviews into dataframe
items = pd.read_csv("sephora_items.csv")
items

preprocessed_items = preprocess_data(items)
preprocessed_items

preprocessed_items.to_csv('sephora_items_processed.csv', index=False)

"""## Sentiment Analysis"""



import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
from vaderSentiment.vaderSentiment import SentimentIntensityAnalyzer

# A function that analyses the sentiment of every reveiw for every product belonging to a particular brand

def sentiment_analyzer_scores(sentence):
    analyser = SentimentIntensityAnalyzer()
    score = analyser.polarity_scores(sentence)
    return score

# Loading the sephora dataset

df=pd.read_csv("sephora_items_processed.csv")
df.head()

# Taking a subset of the original dataframe to perforn sentiment analysis upon

df_sentiment= df[["product_type", "prod_name", "brand", "reviews_combined"]].copy()
df_sentiment.shape

# Dropping all rows that consists of missing reviews

df_sentiment.dropna(axis=0, inplace=True)

# Calling the sentiment analysis function to calculate the sentiment score of every review

lst=[]
for i in df_sentiment["reviews_combined"]:
    lst.append(sentiment_analyzer_scores(i))

# Combining all positive, negative, neutral and compound(combined) sentiments together to be added to the dataframe

dic={}
lst_neg=[]
lst_pos=[]
lst_neu=[]
lst_comp=[]
for i in lst:
    if "neg" in i.keys():
        lst_neg.append(i["neg"])
    if "pos" in i.keys():
        lst_pos.append(i["pos"])
    if "neu" in i.keys():
        lst_neu.append(i["neu"])
    if "compound" in i.keys():
        lst_comp.append(i["compound"])
dic["neg_sent"]=lst_neg
dic["pos_sent"]=lst_pos
dic["neu_sent"]=lst_neu
dic["comp_sent"]=lst_comp
print(dic)
df_sentiment["neg_sent"] =  dic["neg_sent"]
df_sentiment["pos_sent"] =  dic["pos_sent"]
df_sentiment["neu_sent"] =  dic["neu_sent"]
df_sentiment["comp_sent"] =  dic["comp_sent"]

# Fetching only those reviews whose compounded sentiment is greater than 0.95 OR whose positive sentiment>negative sentiment

df_sentiment=df_sentiment[df_sentiment.comp_sent>0.95]
df_sentiment.to_csv("sentiments_data.csv",index=False)

# A dataframe that would consist of ranks based on products

df_sentiment_prod= df_sentiment.copy()
df_sentiment_prod = df_sentiment_prod.groupby(['product_type', 'brand', 'prod_name'],as_index=False).agg(lambda x : x.sum() if x.dtype=='float64' else ' '.join(x))

# A look into the dataframe

df_sentiment_prod.head()

# Calculating the top 5 products for each of the categories- moisturizers and eye-creams, based on compounded sentiment

top5_prods_df= df_sentiment_prod.copy()
top5_prods_df["rank"] = top5_prods_df.groupby(["product_type"])["comp_sent"].rank("max", ascending=False)
top5_prods_df = top5_prods_df.sort_values(['product_type','rank']).groupby('product_type').head(5)
top5_prods_df

"""#### Graph for top 5 eye cream products """

# Top 5 eyecream products

top5_prods_df_eyecream=top5_prods_df.copy()
top5_prods_df_eyecream=top5_prods_df_eyecream.iloc[:5, :]

# Setting the height by plotting neutral sentiment score

ax = top5_prods_df_eyecream.plot(x="prod_name", y="neu_sent", kind="bar", figsize=(15,7), color= "greenyellow")

# Plotting positive and negative sentiment scores on the same axis

fig = top5_prods_df_eyecream.plot(x="prod_name", y="pos_sent", kind="bar", ax=ax, color="mediumaquamarine", figsize=(15,7))
fig = top5_prods_df_eyecream.plot(x="prod_name", y="neg_sent", kind="bar", ax=ax, color="gold", figsize=(15,7), alpha=0.75, rot=90)
fig.set_title("Sentiment scores for top 5 eye creams", pad=25, fontweight="bold", fontdict={'fontsize':17})
fig.set_xlabel('Brand name',fontdict={'fontsize':15}, labelpad=25, fontweight="bold")
fig.set_ylabel('Sentiment score',fontdict={'fontsize':15}, labelpad=15, fontweight="bold")
fig.set_xticklabels(top5_prods_df_eyecream.prod_name, fontdict={'fontsize':13, 'verticalalignment': "center_baseline", 'horizontalalignment': 'center'} )

"""#### Graph for top 5 moisturizer products """

# Top 5 moisturizer products

top5_prods_df_moisturizer =top5_prods_df.copy()
top5_prods_df_moisturizer = top5_prods_df_moisturizer.iloc[5:, :]

# Setting the height by plotting neutral sentiment score

ax = top5_prods_df_moisturizer.plot(x="prod_name", y="neu_sent", kind="bar", figsize=(15,7), color= "greenyellow")

# Plotting positive and negative sentiment scores on the same axis

fig= top5_prods_df_moisturizer.plot(x="prod_name", y="pos_sent", kind="bar", ax=ax, color="mediumaquamarine", figsize=(15,7), title="Sentiment scores for top 5 moisturizers")
fig= top5_prods_df_moisturizer.plot(x="prod_name", y="neg_sent", kind="bar", ax=ax, color="gold", figsize=(20,7), alpha=0.75, rot=90)
fig.set_title("Sentiment scores for top 5 moisturizers", pad=25, fontweight="bold", fontdict={'fontsize':23})
fig.set_xlabel('Brand name',fontdict={'fontsize':20}, labelpad=25, fontweight="bold")
fig.set_ylabel('Sentiment score',fontdict={'fontsize':20}, labelpad=15, fontweight="bold")
fig.set_xticklabels(top5_prods_df_moisturizer.prod_name, fontdict={'fontsize':15})

"""### Brands
###  Top 5 brands based on compounded(combined/overall) sentiment
"""

# Slice of the sentiment's dataframe to calculate ranks' of the brands

df_sentiment_brand= df_sentiment.copy()
df_sentiment_brand = df_sentiment_brand.groupby(['product_type','brand'],as_index=False).agg(lambda x : x.sum() if x.dtype=='float64' else ' '.join(x))

# Calculating the top 5 brands for each of the categories- moisturizers and eye-creams, based on compounded sentiment

top5_brands_df= df_sentiment_brand.copy()
top5_brands_df["rank"] = top5_brands_df.groupby(["product_type"])["comp_sent"].rank("max", ascending=False)
top5_brands_df = top5_brands_df.sort_values(['product_type','rank']).groupby('product_type').head(5)
top5_brands_df

"""#### Graph for top 5 eye cream brands """

# Top 5 eyecream brands

top5_brands_df_eyecream= top5_brands_df.copy()
top5_brands_df_eyecream = top5_brands_df_eyecream.iloc[:5, :]

# Setting the height by plotting neutral sentiment score

ax = top5_brands_df_eyecream.plot(x="brand", y="neu_sent", kind="bar", figsize=(15,7), color= "greenyellow")

# Plotting positive and negative sentiment scores on the same axis

fig= top5_brands_df_eyecream.plot(x="brand", y="pos_sent", kind="bar", ax=ax, color="mediumaquamarine", figsize=(15,7))
fig= top5_brands_df_eyecream.plot(x="brand", y="neg_sent", kind="bar", ax=ax, color="gold", figsize=(15,7), alpha=0.75, rot=45)
fig.set_title("Sentiment scores for top 5 eye creams' brands", pad=25, fontweight="bold", fontdict={'fontsize':17})
fig.set_xlabel('Brand name',fontdict={'fontsize':15}, labelpad=25, fontweight="bold")
fig.set_ylabel('Sentiment score',fontdict={'fontsize':15}, labelpad=15, fontweight="bold")
fig.set_xticklabels(top5_brands_df_eyecream.brand, fontdict={'fontsize':15})

"""#### Graph for top 5 moisturizer brands"""

# Top 5 moisturizer brands

top5_brands_df_moisturizer= top5_brands_df.copy()
top5_brands_df_moisturizer = top5_brands_df_moisturizer.iloc[5:, :]

# Setting the height by plotting neutral sentiment score

ax = top5_brands_df_moisturizer.plot(x="brand", y="neu_sent", kind="bar", figsize=(15,7), color= "greenyellow")

# Plotting positive and negative sentiment scores on the same axis

fig= top5_brands_df_moisturizer.plot(x="brand", y="pos_sent", kind="bar", ax=ax, color="mediumaquamarine", figsize=(15,7))
fig= top5_brands_df_moisturizer.plot(x="brand", y="neg_sent", kind="bar", ax=ax, color="gold", figsize=(15,7), alpha=0.75, rot=45)
fig.set_title("Sentiment scores for top 5 moisturizers' brands", pad=25, fontweight="bold", fontdict={'fontsize':17})
fig.set_xlabel('Brand name',fontdict={'fontsize':15}, labelpad=25, fontweight="bold")
fig.set_ylabel('Sentiment score',fontdict={'fontsize':15}, labelpad=15, fontweight="bold")
fig.set_xticklabels(top5_brands_df_moisturizer.brand, fontdict={'fontsize':15})

"""## Topic Modeling"""



import nltk
nltk.download('stopwords')
nltk.download('wordnet')
import lda
import pandas as pd
from nltk.tokenize import PunktSentenceTokenizer, RegexpTokenizer
from nltk.corpus import stopwords
from nltk.stem import WordNetLemmatizer
from sklearn.feature_extraction.text import CountVectorizer
import numpy as np
import re
from spacy.lang.en import English
from spacy.lang.en.stop_words import STOP_WORDS
from gensim.parsing.preprocessing import remove_stopwords
from nltk.stem import PorterStemmer

import pandas as pd
# uses default alpha and beta values
reviews_df=pd.read_csv("sentiments_data.csv")

#checking for nulls if present any
print("Number of rows with any of the empty columns:")
reviews_df=reviews_df.dropna(axis=0, subset=['reviews_combined'])
print(reviews_df.isnull().sum().sum())

#### Clean the reviews to remove unnecessary stop words
stopwords=set(stopwords.words('english'))
additional_stopwords= ["data", "like", "look", "read", "sephora", "body", "isnt", "know", "wont", "dont", "feel", "try", "test", "felt",
                       "honestly", "start", "brand", "need", "nthis", "doesnt", "havent"]
stopwords = stopwords.union(additional_stopwords)

def clean_review(review):
    words = review.lower().split()
    # remove Stopwords by nltk
    meaningful_words = [w for w in words if not w in stopwords]

    # stopwords removal by spacy (removed since time consuming)
    #meaningful_words = [w for w in meaningful_words if English().vocab[w].is_stop == False]

    # Stemming/lemmitization
    #stemming_words = [stemmer.stem(w) for w in meaningful_words]
    lemma = WordNetLemmatizer()
    ps =PorterStemmer()
    #dont cosider repeated words
    lemm_words = list(set([lemma.lemmatize(w) for w in meaningful_words]))

    # space join words
    sentence = ( ' '.join(lemm_words))
    return remove_stopwords(sentence)

reviews_df['reviews_combined'] = reviews_df['reviews_combined'].apply(clean_review)

# #### get the columns to be used for topic modelling

# # input: prod_name, reviews_combined, 8, (blank)
# product_name = input('provide the column name for product names: ')
# product_review = input('provide the column name for product reviews: ')
# ntopics= input('Provide the number of latent topics to be estimated: ')
# product_brand = input('provide the column name for brand names: ')

product_name ='prod_name'
product_review='reviews_combined'
ntopics=8
product_brand='brand'

word_tokenizer=RegexpTokenizer(r'\w+')
wordnet_lemmatizer = WordNetLemmatizer()
temp = reviews_df[product_review]

def tokenize_text(version_desc):
    lowercase=version_desc.lower()
    text = wordnet_lemmatizer.lemmatize(lowercase)
    tokens = word_tokenizer.tokenize(text)
    return tokens

vec_words = CountVectorizer(tokenizer=tokenize_text,stop_words=stopwords,decode_error='ignore')
total_features_words = vec_words.fit_transform(reviews_df[product_review])

print(total_features_words.shape)

model = lda.LDA(n_topics=int(ntopics), n_iter=500, random_state=1)
model.fit(total_features_words)

topic_word = model.topic_word_
doc_topic=model.doc_topic_
doc_topic=pd.DataFrame(doc_topic)
reviews_df=reviews_df.join(doc_topic)

"""##### Find the top words in each topic"""

stops = ["product", "cream", "hard", "mention", "blue", "current", "zone", "mention", "term", "couple", "think", "retinol", "pour", "idea", "receive", "actually", "fast", "leave",
         "little", "apply", "staple", "version", "love", "better", "nice", "good", "definitely", "minute", "complaint", "enjoy","quickly", "sample"]

topics = ['1_penguins', '2_elephants', '3_pandas', '4_fish', '5_racoon', '6_lizards', '7_birds', '8_dolphins']

n_top_words = 15
vocab = vec_words.get_feature_names_out()
for i, topic_dist in enumerate(topic_word):
    topic_words = np.array(vocab)[np.argsort(topic_dist)][:-n_top_words:-1]
    topic_words = [word for word in topic_words if not word in stops]
    print('{}: {}'.format(topics[i], ' '.join(topic_words)))

topics=pd.DataFrame(topic_word)
topics.columns=vec_words.get_feature_names_out()
topics1=topics.transpose()
topics1.drop(stops, inplace=True)
topics1.to_excel("topic_word_dist.xlsx")

product=pd.DataFrame()
for i in range(int(ntopics)):
    topic="topic_"+str(i)
    product[topic]=reviews_df.groupby(['prod_name'])[i].mean()
product=product.reset_index()
#Remove Null rows
product1 = product.dropna()
product1
#Extract to csv

product1.to_excel("product_topic_dist.xlsx",index=False)



import pyLDAvis
import pyLDAvis.sklearn
pyLDAvis.enable_notebook()
pyLDAvis.sklearn.prepare(model, total_features_words, vec_words)

"""## Lift Ratios"""

reviews_df = pd.read_csv("sentiments_data.csv")
reviews_df.head(5)
reviews_df['brand'] = reviews_df['brand'].str.replace(' ','_')
reviews_df["reviews_combined"] = reviews_df["brand"] + " " + reviews_df["prod_name"] + " " + reviews_df["reviews_combined"]
reviews_df["reviews_combined"]

"""#### Count frequency of brands and attributes"""

# Count frequency of brands and attributes
docs = list(reviews_df["reviews_combined"])

vectorizer = CountVectorizer()
X_count = vectorizer.fit_transform(docs).toarray()
vocab = vectorizer.vocabulary_

feature_names = vectorizer.get_feature_names_out()
df_count = pd.DataFrame(X_count, columns = feature_names)
df_count

brandnames = reviews_df.brand.unique()
brandnames.sort()
brandnames = brandnames.tolist()
brandnames = (map(lambda x: x.replace(" ", "_").lower(), brandnames))
brandnames = list(brandnames)
brands= [ x for x in df_count.columns if x in brandnames]
brands

attributes = [ x for x in df_count.columns if x not in brandnames]
len(attributes)

topic_word = pd.read_excel("topic_word_dist.xlsx", index_col=0)
topic_word = topic_word.index.tolist()
topic_word

attributes=[ x for x in topic_word if x in attributes]
len(attributes)

df_count_a = df_count[attributes]
df_count_a

df_count_a.sum(axis=0).sort_values(ascending=False).to_frame().to_csv("TopWords2.csv")
df_count_a2 = df_count_a.sum(axis=0).sort_values(ascending=False).to_frame().head(20)
df_count_a2



import wordcloud
from wordcloud import WordCloud
wordcloud = WordCloud(width=900,height=500, max_words=150, background_color ='white',relative_scaling=0.5,normalize_plurals=False).generate_from_frequencies(df_count_a.sum(axis=0))

from matplotlib import pyplot as plt
plt.imshow(wordcloud, interpolation='bilinear')
plt.axis("off")
plt.show()

df_count_b = df_count[brands]
df_count_b

df_count_b2 = pd.DataFrame(df_count_b.sum(axis=0).sort_values(ascending=False), columns = ['Count'])
df_count_b2.index.name = "Brand"

top10Brands = df_count_b2[:10]
top10Brands

"""### Calculate Lift Ratios"""

# convert any positive value to 1
def encode_units(x):
    if x <= 0:
        return 0
    if x >= 1:
        return 1

basket_sets = df_count.applymap(encode_units)
basket_sets

basket_sets = basket_sets[top10Brands.index.tolist()]
basket_sets

# convert any positive value to 1
def encode_units(x):
    if x <= 0:
        return 0
    if x >= 1:
        return 1

basket_sets = df_count_b.applymap(encode_units)
basket_sets = basket_sets[top10Brands.index.tolist()]
basket_sets

# to create a matrix of brand_association
data = []
brand_associations = pd.DataFrame(data, columns = top10Brands.index.tolist(), index = top10Brands.index.tolist())
brand_associations_inv = pd.DataFrame(data, columns = top10Brands.index.tolist(), index = top10Brands.index.tolist())

for a in top10Brands.index.tolist():
    for b in top10Brands.index.tolist():
        if a != b:
            a_mentions = basket_sets[a].sum()
            b_mentions = basket_sets[b].sum()
            ab_comentions = ((basket_sets[a] == 1) & (basket_sets[b] == 1)).sum()
            lift = len(basket_sets) * ab_comentions / (a_mentions*b_mentions)
            if lift != 0:
                brand_associations[a][b] = lift
                #brand_associations_inv[a][b] = 1/lift
            else:
                brand_associations[a][b] = 1.00
        else:
            brand_associations[a][b] = 0.00
            #brand_associations_inv[a][b] = 0

brand_associations.to_csv("brand_associations.csv")
brand_associations_inv.to_csv("brand_associations_inv.csv")


pd.options.display.float_format = "{:,.2f}".format
brand_associations

# to identify the top 5 lift pairs
data = []

for a in top10Brands.index.tolist():
    for b in top10Brands.index.tolist():
        if a != b:
            a_mentions = basket_sets[a].sum()
            b_mentions = basket_sets[b].sum()
            ab_comentions = ((basket_sets[a] == 1) & (basket_sets[b] == 1)).sum()
            lift = len(basket_sets) * ab_comentions / (a_mentions*b_mentions)
            my_list = [a, b, lift]
            data.append(my_list)

brand_associations_top = pd.DataFrame(data, columns = ['A','B','lift'])
brand_associations_top = brand_associations_top.sort_values(by = 'lift', ascending = False)
brand_associations_top = brand_associations_top.iloc[::2,:]
brand_associations_top = brand_associations_top.reset_index(drop=True)
brand_associations_top.head(5)

import numpy as np
import seaborn as sns;

np.random.seed(0)
sns.set_theme()
plt.figure(figsize=(10,8))
ax = sns.heatmap(brand_associations.apply(pd.to_numeric).round(decimals=2), cmap="YlGnBu", annot=True)

top5Brands = top10Brands.head(5).index.tolist()
top5Brands

topAttr = df_count_a2.index.tolist()
topAttr

df_count_ab = df_count[top5Brands].join(df_count[topAttr])
df_count_ab

basket_sets = df_count_ab.applymap(encode_units)
basket_sets

attr_brand = pd.DataFrame(0, index=top5Brands, columns=topAttr)
for i in range(len(basket_sets)): #1st row
    for j in range(5): # honda
        for k in range(5,len(basket_sets.columns),1): # geography
            if (basket_sets.iloc[i,j]==1) & (basket_sets.iloc[i,k]==1):
                attr_brand.iloc[j,k-5] = attr_brand.iloc[j,k-5]+1
attr_brand

rslt = pd.DataFrame(np.zeros((0,10)),
                    columns=['top1','top2','top3','top4','top5','top6','top7','top8','top9','top10'])
for i in attr_brand.T.columns:
    df1row = pd.DataFrame(attr_brand.T.nlargest(10, i).index.tolist(),
                          index=['top1','top2','top3','top4','top5','top6','top7','top8','top9','top10']).T
    rslt = pd.concat([rslt, df1row], axis=0)
rslt.index = attr_brand.index
rslt

product_topic = pd.read_excel("product_topic_dist.xlsx", index_col=0)
product_topic.columns = ['1_penguins', '2_elephants', '3_pandas', '4_fish', '5_racoon', '6_lizards', '7_birds', '8_dolphins']
product_topic.head(10)

product_topic_inv = 1/product_topic
product_topic_inv

np.random.seed(0)
sns.set_theme()
plt.figure(figsize=(10,8))
ax = sns.heatmap(product_topic.apply(pd.to_numeric).round(decimals=2), cmap="YlGnBu", annot=True)

rslt = pd.DataFrame(np.zeros((0,5)),
                    columns=['top1','top2','top3','top4','top5'])
for i in product_topic.T.columns:
    df1row = pd.DataFrame(product_topic.T.nlargest(5, i).index.tolist(),
                          index=['top1','top2','top3','top4','top5']).T
    rslt = pd.concat([rslt, df1row], axis=0)
rslt.index = product_topic.index
rslt

# list products under 1_penguins
index = rslt.index
condition = rslt["top1"] == "1_penguins"
indices = index[condition]
indices_list = indices.tolist()
indices_list

# list products under 8_dolphins
index = rslt.index
condition = rslt["top1"] == "8_dolphins"
indices = index[condition]
indices_list = indices.tolist()
indices_list

# list products under 4_fish
index = rslt.index
condition = rslt["top1"] == "4_fish"
indices = index[condition]
indices_list = indices.tolist()
indices_list

"""## Cosine Similarity"""

from scipy import sparse
from sklearn.feature_extraction.text import CountVectorizer
from sklearn.metrics.pairwise import pairwise_distances
import warnings

reviews_df = reviews_df[['prod_name','reviews_combined']]
reviews_df.head(5)

# checking for nulls if present any
print("Number of rows with null values:")
print(reviews_df.isnull().sum().sum())
reviews_df=reviews_df.dropna()

topic_word = pd.read_excel("topic_word_dist.xlsx")
topic_word.columns = ['attributes', '0', '1', '2', '3', '4', '5', '6', '7']
topic_word.head(10)

# reading the attributes from topic modelling
attributes = list(topic_word.attributes)
attributes= " ".join(attributes)
attributes

# merging attibutes to the review
tempDataFrame=pd.DataFrame({'Product_review':[attributes]})
tempDataFrame=tempDataFrame.transpose()
description_list1=reviews_df['reviews_combined']
frames = [tempDataFrame, description_list1]
result = pd.concat(frames)
result.columns = ['review']
result=result.reset_index()

# building bag of words using frequency
vec_words = CountVectorizer(decode_error='ignore')
total_features_words = vec_words.fit_transform(result['review'])
print("The size of the vocabulary space:")
print(total_features_words.shape)

# calculating pairwise cosine similarity
subset_sparse = sparse.csr_matrix(total_features_words)
total_features_review = subset_sparse
total_features_attr = subset_sparse[0,]
similarity = 1-pairwise_distances(total_features_attr,total_features_review, metric='cosine')

# assigning the similarity score to dataframe
#similarity=np.array(similarities[0]).reshape(-1,).tolist()
similarity = pd.DataFrame(similarity)
similarity = similarity.transpose()
similarity.columns = ['similarity']
similarity = similarity.drop(similarity.index[[0]])
reviews_df = reviews_df.assign(similarity = similarity.values)

reviews_df

#writing to an output file
reviews_df.to_excel("similarity_score.xlsx",index=False)

"""## MDS"""

import pandas as pd
import numpy as np
import csv # creates the output csv file

from sklearn.manifold import MDS
from matplotlib import pyplot as plt
import sklearn.datasets as dt
import seaborn as sns
from sklearn.metrics.pairwise import manhattan_distances, euclidean_distances
from matplotlib.offsetbox import OffsetImage, AnnotationBbox

column_names= brand_associations_inv.columns[:]
column_names

brand_associations_inv = pd.read_csv("brand_associations.csv")
brand_associations_inv = brand_associations_inv.drop(columns = ['Unnamed: 0'])
brand_associations_inv

df_mds=brand_associations_inv.set_axis(column_names, inplace=False).set_axis(column_names, axis=1, inplace=False)
brand_associations_top10 = df_mds
brand_associations_top10

### Step 1 - Configure MDS function, note we use default hyperparameter values for this example
from sklearn.manifold import MDS
mds_model=MDS(n_components=2,
          metric=True,
          n_init=4,
          max_iter=300,
          verbose=0,
          eps=0.001,
          n_jobs=None,
          random_state=42,
          dissimilarity='precomputed')

### Step 2 - Fit the data and transform it, so we have 2 dimensions instead of 3
brand_associations1_trans = mds_model.fit_transform(brand_associations_top10)
brand_associations1_trans

### Step 3 - Print stats regarding mds model
print('The new shape of X: ',brand_associations1_trans.shape)
print('No. of Iterations: ', mds_model.n_iter_)
print('Stress: ', mds_model.stress_)

### Step 4 - Convert matrix into dataframe
dis_df = pd.DataFrame(brand_associations1_trans)
dis_df['brands'] = column_names
dis_df

### Step 5 - K-Means clustering on the disimilarity dataframe
from sklearn.preprocessing import StandardScaler
scaler = StandardScaler()
X = dis_df[[0,1]]
X_std = scaler.fit_transform(X)

from sklearn.cluster import KMeans
kmeans = KMeans(init="random",n_clusters=5,n_init=10, max_iter=300,random_state=42)
model = kmeans.fit(X_std)
labels = model.predict(X_std)

### Step 6 - Plot clusters
from matplotlib import pyplot
plt.figure(figsize=(12,12))
pyplot.scatter(dis_df[0], dis_df[1], c=labels, cmap='rainbow')
for i, label in enumerate(dis_df['brands']):
    pyplot.annotate(label, (dis_df[0][i], dis_df[1][i]))



"""## MDS Plot for Product Topic Similarity"""

prod_topic_inv= pd.read_excel("product_topic_dist.xlsx")
prod_topic_inv

prod_name = prod_topic_inv.prod_name
prod_name

prod_topic_inv = prod_topic_inv.drop(columns = ['prod_name'])
prod_topic_inv

#standardize values
#from sklearn.preprocessing import MinMaxScaler
#scaler = MinMaxScaler()
#
from sklearn.preprocessing import StandardScaler
scaler = StandardScaler()
prod_topic_inv = scaler.fit_transform(prod_topic_inv)
prod_topic_inv
#Convert numpy to dataframe
df = pd.DataFrame(prod_topic_inv)

#Add topics as columns
#Convert numpy to dataframe
df = pd.DataFrame(prod_topic_inv)
df1 = df
df1=df1.set_axis(topics, axis=1, inplace=False)
df1

df2 = df.T

df['prod'] = prod_name
df

topics = ['topic0', 'topic1', 'topic2', 'topic3', 'topic4', 'topic5','topic6','topic7']
topics

prod_name = df['prod']
prod_name

df1=df1.set_axis(prod_name, axis=0, inplace=False)
df1

for prod in prod_name:
    df1[prod] = 0

df2=df2.set_axis(topics, axis=0, inplace=False)
df2=df2.set_axis(list(prod_name), axis=1, inplace=False)
df2

for top in topics:
    df2[top] = 0

final_df = pd.concat([df1, df2])
final_df1 = final_df.iloc[:, 8:]
final_df2 = final_df.iloc[:, :8]
final_df3 = pd.merge(final_df1, final_df2, left_index=True, right_index=True)
final_df3

### Step 1 - Configure MDS function, note we use default hyperparameter values for this example
from sklearn.manifold import MDS
mds_model=MDS(n_components=2,
          metric=True,
          n_init=4,
          max_iter=300,
          verbose=0,
          eps=0.001,
          n_jobs=None,
          random_state=42,
          dissimilarity='precomputed')

### Step 2 - Fit the data and transform it, so we have 2 dimensions instead of 3
prod_topic_inv_trans = mds_model.fit_transform(final_df3)#.astype(np.float64)
prod_topic_inv_trans

### Step 3 - Print stats regarding mds model
print('The new shape of X: ',prod_topic_inv_trans.shape)
print('No. of Iterations: ', mds_model.n_iter_)
print('Stress: ', mds_model.stress_)

### Step 4 - Convert matrix into dataframe
dis_df = pd.DataFrame(prod_topic_inv_trans)
dis_df['products'] = final_df3.columns
dis_df

### Step 5 - K-Means clustering on the disimilarity dataframe
from sklearn.preprocessing import StandardScaler
scaler = StandardScaler()
X = dis_df[[0,1]]
X_std = scaler.fit_transform(X)

from sklearn.cluster import KMeans
kmeans = KMeans(init="random",n_clusters=3,n_init=10, max_iter=300,random_state=42)
model = kmeans.fit(X_std)
labels = model.predict(X_std)

### Step 6 - Plot clusters
### Step 6 - Plot clusters
from matplotlib import pyplot
plt.figure(figsize=(12,12))
pyplot.scatter(dis_df[0], dis_df[1], c=labels, cmap='rainbow')
for i, label in enumerate(dis_df['products']):
    pyplot.annotate(label, (dis_df[0][i], dis_df[1][i]))

